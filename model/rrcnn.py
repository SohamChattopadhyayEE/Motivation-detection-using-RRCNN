# -*- coding: utf-8 -*-
"""RRCNN-C_CodeOcean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167CGRsNFqURwajCgsm-S14MRqD-mQybS
"""

## MODEL ##

import torch
import torch.nn as nn

class ResBlock(nn.Module):
  def __init__(self, num_ft = 64, kernel_size = 3, stride = 1, padding = 1):
    super(ResBlock, self).__init__()
    m = []
    for i in range(2):
      m.append(nn.Conv1d(num_ft, num_ft, kernel_size, stride, padding))
      if i==0 :
          m.append(nn.BatchNorm1d(num_ft))
          m.append(nn.ReLU())
    self.body = nn.Sequential(*m)

  def forward(self, x):
    res = self.body(x)
    res += x
    return res


class RRCNN_C(nn.Module):
  def __init__(self, num_channels,  num_classes, num_res_ft = 64, num_res = 2):
    super(RRCNN_C, self).__init__()
    self.conv = nn.Conv1d(num_channels, num_res_ft, kernel_size = 3, stride = 1, padding = 1)
    self.res = ResBlock()
    mat = []
    for _ in range(num_res):
      mat.append(ResBlock(num_ft = num_res_ft))
      mat.append(nn.RReLU())
    self.res_body_1 = nn.Sequential(*mat) 

    mat2 = []
    for _ in range(num_res):
      mat2.append(ResBlock(num_ft = num_res_ft))
      mat2.append(nn.RReLU())
    self.res_body_2 = nn.Sequential(*mat2) 

    mat3 = []
    for _ in range(num_res):
      mat3.append(ResBlock(num_ft = num_res_ft))
      mat3.append(nn.RReLU())
    self.res_body_3 = nn.Sequential(*mat3) 

    self.avg = nn.AdaptiveAvgPool1d(1)
    self.maxpool = nn.MaxPool1d(1)

    self.fc = nn.Linear(num_res_ft, num_classes)
    self.clf = nn.Softmax()

  def forward(self, x):
    x_in = self.conv(x)
    x = self.res_body_1(x_in)
    x = x + x_in
    x1 = x
    x = self.res_body_2(x)
    x = x+x1
    x2 = x
    x = self.res_body_3(x)
    x = x + x2
    x = self.avg(x)
    x = torch.flatten(x)
    x = self.fc(x)
    x = self.clf(x)
    return x

import pickle
with open('/content/drive/My Drive/Data/Data.pkl', 'rb') as f:
    df_nve = pickle.load(f)

with open('/content/drive/My Drive/Data/Data_E1_pve.pkl', 'rb') as f:
    df_pve = pickle.load(f)

#Alpha signal
import numpy as np
data_nve = []
for i in range(len(df_nve)):
  data_2_channels = []
  data_2_channels.append(np.array(df_nve[0])[i].transpose()[12])#F3
  data_2_channels.append(np.array(df_nve[0])[i].transpose()[7])#F4
  data_2_channels.append(np.array(df_nve[0])[i].transpose()[11])#F7
  data_2_channels.append(np.array(df_nve[0])[i].transpose()[9])#F8
  #data_2_channels.append(np.array(df_nve[0])[i].transpose()[6])#Fz
  #data_2_channels.append(np.array(df_nve[0])[i].transpose()[14])#T7
  #data_2_channels.append(np.array(df_nve[0])[i].transpose()[3])#T8
  data_nve.append(np.array(data_2_channels).transpose())


data_pve = []
for i in range(len(df_pve)):
  data_2_channels = []
  data_2_channels.append(np.array(df_pve[0])[i].transpose()[12])#F3
  data_2_channels.append(np.array(df_pve[0])[i].transpose()[7])#F4
  data_2_channels.append(np.array(df_pve[0])[i].transpose()[11])#F7
  data_2_channels.append(np.array(df_pve[0])[i].transpose()[9])#F8
  #data_2_channels.append(np.array(df_pve[0])[i].transpose()[6])#Fz
  #data_2_channels.append(np.array(df_pve[0])[i].transpose()[14])#T7
  #data_2_channels.append(np.array(df_pve[0])[i].transpose()[3])#T8
  data_pve.append(np.array(data_2_channels).transpose())

import pandas as pd
df = [df_nve, df_pve]
df = pd.concat(df,ignore_index=True)
print(df)



label_pve = []
for i in range(len(df_pve)):
  label_pve.append(0)
print('pve: ', len(label_pve))


label_nve = []
for i in range(len(df_nve)):
  label_nve.append(1)
print('nve: ', len(label_nve))

labels = label_nve + label_pve
print('label: ',len(labels))

pve_test_points = 40
nve_test_points = 60
#pve_train_points = 
nve_train_points = 300

label_nve_test = label_nve[0:nve_test_points]
print('label_nve_test',len(label_nve_test))
label_pve_test = label_pve[0:pve_test_points]
print('label_pve_test',len(label_pve_test))
df_pve_test =  np.array(data_pve[0:pve_test_points])
print('df_pve_test',len(df_pve_test))
df_nve_test =  np.array(data_nve)[0:nve_test_points]
print('df_nve_test',len(df_nve_test))


label_nve_train = label_nve[nve_test_points:nve_train_points]
print('label_nve_train',len(label_nve_train))
label_pve_train = label_pve[pve_test_points:]
print('label_pve_train',len(label_pve_train))
df_pve_train =  np.array(data_pve)[pve_test_points:]
print('df_pve_train',len(df_pve_train))
df_nve_train =  np.array(data_nve)[nve_test_points:nve_train_points]
print('df_nve_train',len(df_nve_train))

train_data = np.concatenate((df_nve_train, df_pve_train))
print('train_data: ', len(train_data))
train_label = np.concatenate((label_nve_train, label_pve_train))
print('train_label: ', len(train_label))
test_data = np.concatenate((df_nve_test, df_pve_test))
print('test_data: ', len(test_data))
test_label = np.concatenate((label_nve_test, label_pve_test))
print('test_label: ', len(test_label))

## Model Train and Validation 

num_epoch = 600
lr = 0.0001
momentum = 0.5155397451642598
num_channels = 4
num_residual_features = 64
num_resedual_blocks = 7
num_classes = 2
model = RRCNN_C(num_channels = num_channels,  num_classes = num_classes, 
                num_res_ft = num_residual_features, num_res = num_resedual_blocks)

model = model.cuda()
optimizer = torch.optim.Adam(model.parameters(), lr = lr)
criterion = nn.CrossEntropyLoss()

for epoch in range(num_epoch):
  train_loss = 0.0
  correct = total = 0
  for i in range(len(train_data)):
    optimizer.zero_grad()
    data_point, label = torch.tensor(train_data[i]), torch.tensor(np.array([train_label[i]]))
    data_point, label = data_point.cuda(), label.cuda()
    data_point = data_point.reshape(1,num_channels,-1)
    output = model(data_point.float())
    loss = criterion(output.reshape(1,-1), label)
    loss.backward()
    optimizer.step()
    train_loss += loss.item()
    _, predicted = torch.max(output.reshape(1,-1).data, 1)
    total += label.size(0)
    correct += (predicted == label).sum().item()

  print('Training Epoch: ', epoch)
  print('training loss: ', train_loss)
  print('Accuracy: ', 100*correct/total)

  with torch.no_grad():
    val_loss = 0.0
    total = correct = 0
    for j in range(len(test_data)):
      val_data, val_label = torch.tensor(test_data[j]), torch.tensor(np.array([test_label[j]]))
      val_data, val_label = val_data.cuda(), val_label.cuda()
      val_data = val_data.reshape(1,num_channels,-1)
      out_val = model(val_data.float())
      loss = criterion(out_val.reshape(1,-1), val_label)
      val_loss += loss.item()
      _, predicted_val = torch.max(out_val.reshape(1,-1).data, 1)
      total += val_label.size(0)
      correct += (predicted_val == val_label).sum().item()
  print('Validation Loss: ', val_loss)
  print('Validation Accuracy: ', 100*correct/total)
